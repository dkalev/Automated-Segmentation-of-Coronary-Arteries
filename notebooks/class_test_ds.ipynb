{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "import os\n",
    "import nrrd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append('..')\n",
    "from data_utils.helpers_classification import get_pos_coords, get_neg_coords, get_nearly_pos_coords, get_vol_hard_mask, get_hard_neg_coords, get_vol_paths, normalize_vols"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../dataset/classification/dataset.json', 'r') as f:\n",
    "    meta = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_meta = { k:v for k,v in meta.items() if k != 'vol_meta' }\n",
    "test_meta['N'] = 20000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "valid_meta = { k:v for k,v in meta['vol_meta'].items() if v['split'] == 'valid' }"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for vol_id, vol_meta in valid_meta.items():\n",
    "    vol_meta['patches'] = set( tuple(coords[:-1]) for coords in vol_meta['patches'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vol_paths = get_vol_paths('../dataset/raw/ASOCA2020Data/')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vol_paths = [(x[0],\n",
    "  Path(f'../dataset/classification/{meta[\"vol_meta\"][str(x[0])][\"split\"]}/vols/{x[0]}.npy'),\n",
    "  *x[2:]\n",
    " ) for x in vol_paths]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_patches = 20000\n",
    "patch_size = 68"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from typing import Callable\n",
    "from functools import partial"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def sample_new(sample_fn:Callable, n_samples:int, already_sampled: set)->np.ndarray:\n",
    "    res = set()\n",
    "    while len(res) < n_samples:\n",
    "        samples = sample_fn()\n",
    "        label = samples[0][-1]\n",
    "        samples = set([tuple(coord[:-1]) for coord in samples.tolist()])\n",
    "        samples = samples - already_sampled\n",
    "        res = res.union(samples)\n",
    "    return np.array([ [*x, label] for x in res ])[:n_samples].astype(int)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert n_patches % 4 == 0\n",
    "res = {}\n",
    "n_per_vol = n_patches // len(valid_meta)\n",
    "\n",
    "for vol_id, vol_path, targ_path, heart_mask_path in tqdm(vol_paths):\n",
    "    if str(vol_id) not in valid_meta: continue\n",
    "    vol           = np.load(vol_path)\n",
    "    targs, _      = nrrd.read(targ_path, index_order='C')\n",
    "    heart_mask, _ = nrrd.read(heart_mask_path, index_order='C')          \n",
    "\n",
    "    targs = targs.astype(np.uint8)\n",
    "    heart_mask = heart_mask.astype(np.uint8)\n",
    "    \n",
    "    already_sampled = valid_meta[str(vol_id)]['patches']\n",
    "\n",
    "    n_pos = n_per_vol//2\n",
    "    pos_coords = sample_new(\n",
    "        partial(get_pos_coords, targs, patch_size, n_samples=n_pos),\n",
    "        n_samples=n_pos,\n",
    "        already_sampled=already_sampled\n",
    "    )\n",
    "    \n",
    "    n_neg_rand = n_per_vol//10\n",
    "    neg_coords = sample_new(\n",
    "        partial(get_neg_coords, targs, heart_mask, patch_size, n_neg_rand),\n",
    "        n_samples=n_neg_rand,\n",
    "        already_sampled=already_sampled\n",
    "    )\n",
    "    \n",
    "    n_near_pos = 2*n_per_vol//10\n",
    "    neg_near_pos_coords = sample_new(\n",
    "        partial(get_nearly_pos_coords, targs, patch_size, n_near_pos, offset=8),\n",
    "        n_samples=n_near_pos,\n",
    "        already_sampled=already_sampled\n",
    "    )\n",
    "    \n",
    "    vol_hard_mask = get_vol_hard_mask(vol, targs, heart_mask)\n",
    "    hard_neg_coords = get_hard_neg_coords(vol_hard_mask, targs, already_sampled, patch_size, 2*n_per_vol//10)\n",
    "\n",
    "    coords = np.vstack((pos_coords, neg_near_pos_coords, neg_coords, hard_neg_coords)).astype(int)\n",
    "    coords = np.random.permutation(coords)\n",
    "    res[vol_id] = coords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for vol_id in res:\n",
    "    coords = set([tuple(x[:-1]) for x in res[vol_id]])\n",
    "    already_sampled = valid_meta[str(vol_id)]['patches']\n",
    "    assert len(coords.intersection(already_sampled)) == 0, vol_id"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_meta['vol_meta'] = {k: {\n",
    "        'split': 'valid',\n",
    "        'n_patches': len(v),\n",
    "        'patches': v.tolist(),\n",
    "    } for k,v in res.items()}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../dataset/classification/dataset_test.json', 'w') as f:\n",
    "    json.dump(test_meta, f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from data_utils.datamodule import AsocaClassificationDataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../dataset/classification/dataset_test.json', 'r') as f:\n",
    "    test_meta = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds = AsocaClassificationDataset(\n",
    "    ds_path='../dataset/classification',\n",
    "    meta_fname='dataset_test.json',\n",
    "    split='valid')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ds.file_ids"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('../dataset/classification/dataset.json', 'r') as f:\n",
    "    meta = json.load(f)\n",
    "with open('../dataset/classification/dataset_test.json', 'r') as f:\n",
    "    meta_test = json.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from collections import Counter"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for vol_id in meta['vol_meta']:\n",
    "    print(vol_id, \n",
    "          meta['vol_meta'][vol_id]['n_patches'],\n",
    "          len(set([tuple(x[:-1]) for x in meta['vol_meta'][vol_id]['patches']])),\n",
    "          Counter([x[-1] for x in meta['vol_meta'][vol_id]['patches']])\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for vol_id in meta_test['vol_meta']:\n",
    "    coords_valid = set([tuple(x[:-1]) for x in meta['vol_meta'][vol_id]['patches']])\n",
    "    coords_test = set([tuple(x[:-1]) for x in meta_test['vol_meta'][vol_id]['patches']])\n",
    "    print(vol_id, \n",
    "          meta_test['vol_meta'][vol_id]['n_patches'],\n",
    "          len(coords_test),\n",
    "          len(coords_test.intersection(coords_valid)),\n",
    "          Counter([x[-1] for x in meta_test['vol_meta'][vol_id]['patches']])\n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import wandb\n",
    "import torch\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from train import get_class\n",
    "from data_utils.datamodule import AsocaClassificationDataModule\n",
    "import pytorch_lightning as plt\n",
    "\n",
    "\n",
    "data_dir='../dataset/classification'\n",
    "model_dir = '/var/scratch/ebekkers/damyan/models'\n",
    "runs = wandb.Api().runs(path='ASOCA_final', filters={\n",
    "    'config.seed': {'$in': [0,11,42]},\n",
    "    'config.model/model': {\"$regex\": \"models.classification.*\"}, \n",
    "    })\n",
    "\n",
    "# trainer = plt.Trainer(gpus=4, accelerator='ddp', replace_sampler_ddp=False)\n",
    "# trainer = plt.Trainer(gpus=1)\n",
    "dm = AsocaClassificationDataModule(data_dir=data_dir)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run = [run for run in runs if run.name == 'gallant-sun-765'][0]\n",
    "model_params = { k.split('/')[-1]:v for k,v in run.config.items() if 'model' in k }\n",
    "class_name = model_params['model']\n",
    "del model_params['model']\n",
    "if 'initialize' in model_params: model_params['initialize'] = False\n",
    "\n",
    "with run.files()[0].download(model_dir, replace=True)as model_f:\n",
    "    model: plt.LightningModule = get_class(class_name)(**model_params)\n",
    "    with open(model_f.name, 'rb') as f:\n",
    "        ckpt = torch.load(f)\n",
    "    state_dict = {k:v for k,v in ckpt['state_dict'].items() if 'in_indices_' not in k}\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    all_preds = torch.empty(20000)\n",
    "    all_targs = torch.empty(20000)\n",
    "    bs = 32\n",
    "    for i, x, targs in enumerate(dm.test_dataloader(batch_size=bs)):\n",
    "        x = x.cuda()\n",
    "        preds = model(x).cpu().detach()\n",
    "        all_preds[i*bs:i*bs+bs] = preds\n",
    "        all_targs[i*bs:i*bs+bs] = targs\n",
    "        "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "shutil.rmtree(f'{model_dir}/{run.project}/{run.id}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mp = np.load('../manual_preds.npy')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds, targs = mp[0], mp[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "preds = preds.round()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "acc = (preds == targs).mean()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "acc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tp = np.sum((preds == targs) & (preds == 1.))\n",
    "tn = np.sum((preds == targs) & (preds == 0.))\n",
    "fp = np.sum((preds != targs) & (preds == 1.))\n",
    "fn = np.sum((preds != targs) & (preds == 0.))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert sum([tp, tn, fp, fn]) == 20000"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tp, tn, fp, fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "precision = tp / (tp+fp)\n",
    "recall = tp / (tp + fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "precision, recall"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f1 = 2 * precision * recall / (precision + recall)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('asoca': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "722564113454f0579360dc6418f2dffdf0822e294a9a4a0e8df86f8328e5ca9e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}